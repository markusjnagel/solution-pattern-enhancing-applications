[#workshop_add_streaming]
== Adding Event Streaming capabilities to the application

=== Introduction

To support the business requirement of capturing and processing user activity on the Globex Coolstuff application, two new services have been developed:

* *Activity Tracking service*: This service exposes a REST endpoint. User activities on the Coolstuff website (such as opening a product page, liking a product etc..) generates an activity payload which is sent to the Activity tracking REST endpoint. The service transforms this payload into a Kafka message which is sent to a topic on the Kafka broker.
* *Recommendation Engine*: This service consumes and processes the events produced by the Activity Tracking service. The service uses the Kafka Streams library to continuously determine the top featured products (the products which generate the most activities).
The service also exposes a REST endpoint to expose the list of featured products.

Both services are developed using Quarkus and the Quarkus extensions for reactive messaging and Kafka Streams. The development of the services is outside the scope of this workshop, but you are encouraged to examine the source code of the applications on GitHub: link:https://https://github.com/app-modernization-workshop-globex/activity-tracking-service[Activity Tracking Service] and link:https://github.com/app-modernization-workshop-globex/recommendation-engine[Recommendation Engine]

For the Kafka platform, the workshop uses Red Hat OpenShift Streams for Apache Kafka. OpenShift Streams for Apache Kafka is a fully hosted and managed Kafka cloud service. The service fulfills the requirement of Globex to avoid having to install and maintain a streaming platform. The service can be very easily consumed and used by developers.

In this workshop, you use link:https://servicebinding.io/[Service Binding For Kubernetes]. Service Binding allows you to communicate connection details and secrets to an application to allow it to bind to a service. In this context, a service can be anything: a Kafka instance, a NoSQL database, etc. By using Service Binding, we no longer need to configure connection details (host, port), authentication mechanisms (SASL, OAuth) and credentials (username/password, client id/client secret) in an application. Instead, Service Binding injects these variables into your application container (as files or environment variables) for your application to consume. The Quarkus Kubernetes Service Binding extension enables Quarkus applications to automatically pickup these variables, injected as files, from the container's filesystem, removing the need to specify any configuration settings in the application resources (e.g configuration files) themselves.

=== Activities

In this part of the workshop you will:

* Provision and configure an OpenShift Streams for Apache Kafka instance.
* Connect the Activity Tracking and Recommendation Engine applications to the OpenShift Streams for Apache Kafka instance using link:https://docs.openshift.com/container-platform/4.10/applications/connecting_applications_to_services/understanding-service-binding-operator.html[Service Binding].


=== Environment and prerequisites

The Globex Coolstuff application is deployed on an OpenShift cluster in a separate namespace. All activities in this part of the workshop can be done in the OpenShift console and do not require any tools to be installed on your workstation. However, the setup and the configuration of the Streams for Apache Kafka instance as well as the service binding can also be done using the Red Hat OpenShift Application Services (`rhoas`) CLI. Instructions for completing the workshop using the `rhoas` CLI can be found in the appendix at the end of the instructions.

Prerequisites:

* A Red Hat Account
* Optional: `rhoas` and `oc` CLI installed on your workstation.

=== Step-By-Step Instructions

==== Explore the environment

. In a browser window, navigate to the console of your OpenShift cluster. Open the *Developer* perspective in the *globex* namespace.
. In the Developer perspective, open the *Topology* view. Expect to see something like this (rearrange the topology as you see fit):
+
image::04/globex-deployment-topology.png[]
+
The deployed topology consists of:
+
** `globex-ui`: The Globex Coolstuff web application (Node.js/Angular). 
** `catalog-app`: The Globex Coolstuff catalog service, consisting of the catalog database and the Spring Boot catalog microservice.
** `inventory-app`: The Globex Coolstuff inventory service, consisting of the inventory database and the Quarkus inventory microservice.
** `activity-tracking`: The Activity Tracking service. Notice that the deployment of the service is scaled to zero. The service will be scaled up once the connection to the Kafka broker is set up.
**  `recommendation-engine`: The Recommendation Engine service. Notice that the deployment of the service is scaled to zero. The service will be scaled up once the connection to the Kafka broker is set up.
** `activity-tracking-simulator`: A Quarkus service that simulates user activity events and sends them to the Activity Tracking service.
. Find the route to the *Globex UI* application and open the URL in your browser.. Expect to see the home page of the Globex Coolstuff web application:
+
image::04/globex-coolstuff-home-page.png[]
+
Click on *Cool Stuff Store* in the top menu to see a paginated list of products:
+
image::04/globex-coolstuff-product-page.png[]
+
Note the _Featured_ pane on the home page, which is empty at the moment. Also the product list page has an empty bar above the product list. These elements will be populated once the recommendation engine is up and running. 

==== Provision and configure a Streams for Apache Kafka instance

In this step you provision a Streams for Apache Kafka instance and create a topic for the activity tracking events.

The instructions use the Red Hat Hybrid Cloud Console at link:https://console.redhat.com[console.redhat.com]. Instructions for the `rhoas` CLI can be found in the appendix.

*Provision a Kafka instance in OpenShift Streams for Apache Kafka*

. Navigate to https://console.redhat.com[console.redhat.com] and log in with your Red Hat account credentials.

. On the https://console.redhat.com[console.redhat.com] landing page, select *Application and Data Services* from the menu on the left.

. On the Application Services landing page, select *Streams for Apache Kafka → Kafka Instances*.
+
image::04/console-redhat-com-kafka-instances.png[]

. On the Kafka Instances overview page, click the *Create Kafka* instance button. Enter a unique name and select the relevant _Cloud region_ for your Kafka instance and click *Create instance*. This starts the provisioning process for your Kafka instance.
+
[NOTE]
====
This will create a evaluation Kafka instance, which will remain available for 48 hrs. The Kafka instance comes with some limitations, which are listed in the *Create instance* window. The eval Kafka instance consists of a single broker, while production Kafka brokers have a minimum of 3 brokers. 
====

. The new Kafka instance is listed in the instances table. After a couple of minutes, your instance should be marked as ready. 
+
image::04/console-redhat-com-kafka-instance-ready.png[]

. When the instance _Status_ is _Ready_, you can start using the Kafka instance. You can use the options icon (three vertical dots) to view, connect to, or delete the instance as needed.

Normally, when using Streams for Apache Kafka, the next steps would be to create a service account and set up the Access Control List for that service account. However, when using Service Binding to connect applications to the Kafka instance, a service account is created as part of the binding. Once the service account is created, you will need to setup the required permissions for that service account. An alternative is to set up wildcard permissions (valid for all service accounts), but this is generally considered less secure.  

*Create a Kafka Topic in OpenShift Streams for Apache Kafka*

After you create a Kafka instance, you can create Kafka topics to start producing and consuming messages in your services.

The Activity Tracking service sends activity events to a topic name `globex.tracking`. Additional topics ae required by the recommendation engine, but these topics are created dynamically when the application starts up.

. In the *Kafka Instances* page of the web console, click the name of the Kafka instance that you want to add a topic to.

. Select the *Topics* tab, click *Create topic*, and follow the guided steps to define the topic details. Click *Next* to complete each step and click *Finish* to complete the setup.
+
image::04/rhosak-create-topic.png[]
+
* *Topic name*: Enter `globex.tracking`.
* *Partitions*: Set the number of partitions for this topic. For this workshop, keep the number of partitions to 1. +
Partitions are distinct lists of messages within a topic and enable parts of a topic to be distributed over multiple brokers in the cluster. A topic can contain one or more partitions, enabling producer and consumer loads to be scaled.
* *Message retention*: Set the message retention time and size to the relevant value and increment. The default retention time is set to `A week` and the retention size to `Unlimited`. Message retention time is the amount of time that messages are retained in a topic before they are deleted or compacted, depending on the cleanup policy. Retention size is the maximum total size of all log segments in a partition before they are deleted or compacted. For this workshop you can keep the default values.
* *Replicas*: For this release of Streams for Apache Kafka, the replicas are preconfigured. As the eval Kafka instance consists of only one broker, the number of partition replicas for the topic is set to `1`, as well as the minimum number of follower replicas that must be in sync with a partition leader. For a production Kafka broker on Streams for Apache Kafka these values will be `3` and `2` respectively. +
Replicas are copies of partitions in a topic. Partition replicas are distributed over multiple brokers in the cluster to ensure topic availability if a broker fails. When a follower replica is in sync with a partition leader, the follower replica can become the new partition leader if needed.

. After you complete the topic setup, the new Kafka topic is listed in the topics table. You can now start producing and consuming messages to and from this topic using services that you connect to this instance.
+
image::04/rhosak-topic-created.png[]

==== Binding applications to Streams for Apache Kafka

Binding applications to services using Service Binding requires the Service Binding operator to be installed on the OpenShift cluster. To bind more specifically to a OpenShift Streams for Apache Kafka instance, the Red Hat OpenShift Application Services (RHOAS) operator is required. Both operators have been installed on your OpenShift cluster.

*Connect OpenShift Streams for Apache Kafka*

In this part of the workshop you connect your OpenShift instance to the Streams for Kafka instance you created previously. This can be done from the Developer perspective on the OpenShift console, or using the `rhoas` CLI. Instructions for the CLI can be found in the appendix.

. In a browser window, navigate to the console of your OpenShift cluster. Open the *Developer* perspective in the *globex* namespace.
. In the Developer perspective, navigate to the *+Add* view. Locate the *Developer Catalog* card with the *Managed Services* entry
+
image::04/openshift-console-developer-catalog.png[]
. Click the *Managed Services* link. This opens the Managed Services page, which has a card for *Red Hat OpenShift Application Services*.
+
image::04/openshift-console-application-services.png[]
. In order to discover the managed services you are entitled to, you need to unlock the functionality with a token obtained from link:https://console.redhat.com[console.redhat.com]. +
Open a new browser tab and navigate to link:https://console.redhat.com/openshift/token[console.redhat.com/openshift/token]. Click on *Load token* in the *Connect with offline token* box. Copy the generated API token.
. Go back to the browser tab with the OpenShift console, and click the *Red Hat OpenShift Application Services* card. Paste the API token value in the *API Token* field. Click *Connect*. +
You are redirected back to the *Managed Services* page, which shows now a card for *Red Hat OpenShift Streams for Apache Kafka*.
+
image::04/openshift-console-rhosak.png[]
. Click the *Red Hat OpenShift Streams for Apache Kafka* card, and click *Connect*. This opens a page which shows the Kafka instances that you can connect to. Select the entry corresponding to the Kafka instance you created previously. Click *Next*
+
image::04/openshift-console-rhosak-connect.png[]
. You are redirected to the *Topology View* of the Developer perspective, which shows now an entry for the managed Kafka instance.
+
image::04/openshift-console-topology-rhosak.png[]
. The entry is backed by a `KafkaConnection` custom resource created by the OpenShift Application Services operator. To see the details of the KafkaConnection resource, click on the resource in the Topology view, and in the Details window, select *Edit KafkaConnection* to see the YAML structure of the custom resource. +
Notice that the YAML structure contains the bootstrap URL to the Kafka broker, as well as a reference to a secret containing the data of a service account, named `rh-cloud-services-service-account`.

*Set Permissions for a Service Account*

As part of connecting to the managed Kafka instance, a service account is created. This is the service account that will be used by the Activity Tracking and Recommendation Engine services to actually connect to the managed Kafka instance. To make this work, the service account needs permissions, in particular the service account needs to be able to consume from topics, produce to topics and create new topics.

Setting permissions in the Access Control List of a Streams for Apache Kafka can be done in the link:https://console.redhat.com[console.redhat.com] console, or using the `rhoas` CLI. Instructions for the CLI can be found in the appendix.

. Navigate to the *Application and Data Services* page of the link:https://console.redhat.com[console.redhat.com] console.
. On the *Service Accounts* page, check that a service account was created by the OpenShift Application Services operator. Look for a service account with a name like `rhoas-operator-xxx`.
. Navigate to the *Streams for Apache Kafka -> Kafka instances* page and open the page for your Kafka instance.
. Click the *Access* tab to view the current ACL for this instance.
+
image::04/rhosak-default-access.png[]

. Click *Manage access*, use the *Account* drop-down menu to select the service account that was created by the OpenShift Application Services operator, and click *Next*.

. Under *Assign Permissions*, use the drop-down menus to set the permissions shown in the following table for this service account. +
Select the *Consume from a topic* and *Produce to a topic* from the *Task-based permission* possibilities. Set the topic and consumer group names to `is` and `*`.
+
image::04/rhosak-manage-access.png[]
+
Click *Save*.
+
The ACL list for the service account should look like:
+
image::04/rhosak-access-serviceaccount.png[]

*Bind applications to Streams for Apache Kafka*

You can now bind the Activity Tracking Service and Recommendation Engine to the OpenShift Streams for Apache instance. Through Service Binding the connection details are injected into the application pods. Service Binding to a managed Kafka instance can be done on the Topology view of OpenShift console, or through the `rhoas` CLI. The instructions for the `rhoas` CLI can be found in the appendix.

. Navigate to the *Topology* view of the OpenShift console in the *globex* namespace.
. Hover over the *activity-tracking* deployment, and grab the arrow that appears. Drag the arrow to the *KafkaConnection* icon. When reaching the KafkaConnection icon, a text box `Create Service Binding` appears. Release the arrow. Click *Create* in the *Create Service Binding* pop-up window. The Activity Tracking deployment and the KafkaConnection icon are now connected with a solid black arrow.
+
image::04/rhosak-service-binding.png[]
. Click on the activity-tracking deployment to open the details window, and click on the deployment name to open the full details of the Deployment. Notice that the service binding occurs by injecting a secret into the pod:
+
image::04/service-binding-secret.png[]
. Scale the activity-tracking deployment to 1 replica.
. Check the logs of the activity-tracking pod, and notice that the pod successfully connects to the Kafka broker instance.
+
----
exec java -Dquarkus.http.host=0.0.0.0 -Djava.util.logging.manager=org.jboss.logmanager.LogManager -XX:+ExitOnOutOfMemoryError -cp . -jar /deployments/quarkus-run.jar
__  ____  __  _____   ___  __ ____  ______ 
 --/ __ \/ / / / _ | / _ \/ //_/ / / / __/ 
 -/ /_/ / /_/ / __ |/ , _/ ,< / /_/ /\ \   
--\___\_\____/_/ |_/_/|_/_/|_|\____/___/   
2022-05-23 15:26:40,829 INFO  [org.apa.kaf.com.sec.aut.AbstractLogin] (main) Successfully logged in.
2022-05-23 15:26:41,061 INFO  [io.sma.rea.mes.kafka] (main) SRMSG18258: Kafka producer kafka-producer-tracking-event, connected to Kafka brokers 'globex-ca-m-q-mtp---qgalcrg.bf2.kafka.rhcloud.com:443', is configured to write records to 'globex.tracking'
2022-05-23 15:26:41,363 INFO  [io.quarkus] (main) activity-tracking-service 1.0.0-SNAPSHOT on JVM (powered by Quarkus 2.7.4.Final) started in 2.427s. Listening on: http://0.0.0.0:8080
2022-05-23 15:26:41,364 INFO  [io.quarkus] (main) Profile prod activated. 
2022-05-23 15:26:41,364 INFO  [io.quarkus] (main) Installed features: [cdi, kafka-client, resteasy-reactive, smallrye-context-propagation, smallrye-health, smallrye-reactive-messaging, smallrye-reactive-messaging-kafka, vertx]
----
. Repeat the same procedure for the *recommendation-engine* deployment. Once the service binding created, scale the deployment to 1 pod.
. Once the recommendation-engine is up and running, check in the link:https://console.redhat.com[console.redhat.com] console that a number of new topics have been created:
+
image::04/rhosak-kafka-streams-topics.png[]
+
Those are the topics created by the Kafka Streams topology in the Recommendation Engine to calculate the top featured products based on activity events.

==== Testing the Globex Coolstuff application

Now that the Activity Tracking and Recommendation Engine apps are up and running, we can test the generation of activity events and the calculation of the top featured products.

The deployment topology for the workshop includes an activity simulator service which will generate a number of activity events randomly distributed over a list of products. These activity events are sent to the Activity Tracking service and transformed into Kafka messages in the `globex.tracking` topic. These messages are consumed by the Recommendation Engine app to calculate the top featured products.

. In the OpenShift console, locate the route for the *activity-tracking-simulator* deployment.
. Open a browser tab pointing to the application, and navigate to the `q/swagger-ui` path. This opens a Swagger UI page which allows you to use the REST API of the application. The REST application has only one operation, `POST /simulate`.
+
image::04/activity-tracking-simulator-swagger-ui.png[]
. Generate a number activities. Set `count` to any value between 100 and 1000.
. OpenShift Streams for Apache Kafka has a message viewer functionality that allows you to inspect the contents of messages in a topic. +
Navigate to link:https://console.redhat.com[console.redhat.com], select your Kafka instance and in the instance window select the *Topics* tab. Click on the `globex.tracking` topic, and select the messages tab. Notice the activity event messages, with a JSON payload:
+
image::04/rhosak-messages-tracking.png[]
. The featured product list calculated by the Recommendation Engine is produced to the `globex.recommendation-product-score-aggregated-changelog` topic. The list is recalculated roughly every 10 seconds as long as activity events are produced. Every calculation produces a message to the changelog topic. The last message in the topic represents the latest top featured list.
+
image::04/rhosak-messages-aggregated-changelog.png[]
. In a browser window, navigate to the home page of the Globex Coolstuff web application. Notice that the home page now shows a list of featured products.
+
image::04/globex-coolstuff-home-page-featured.png[]
+
Also, the product page now shows a banner with the featured products.
+
image::04/globex-coolstuff-product-page-featured.png[]

Congratulations! You reached the end of this part of the workshop, in which you added event streaming capabilities to the Globex Coolstuff application, using the OpenShift Streams for Apache Kafka managed cloud service, and Service Binding to connect your apps the the Kafka instance. 

:sectnums!:

==== Appendix: Use the `rhoas` CLI

If you prefer to use the `rhoas` CLI to provision and configure the OpenShift Streams for Apache Kafka instance, and to bind your applications to the Kafka instance using Service Binding, you can follow the following instructions:

* Install the `rhoas` CLI
** Obtain the latest release of the `rhoas` CLI archive for your operating system from the https://github.com/redhat-developer/app-services-cli/releases/latest[Red Hat OpenShift Application Services CLI releases] page on GitHub.
** Install the package (or extract the archive), and add the `rhoas` executable to your path.
** Check the version of the CLI
+
[.console-input]
[source,bash]
----
rhoas version
----
+
[.console-output]
[source,text]
----
rhoas version 0.42.2
----

* Login into Red Hat Application Services
+
[.console-input]
[source,bash]
----
rhoas login
----
+
This initiates a browser based login. Log in using your Red Hat Account credentials.

* Provision an evaluation Kafka instance:
** Provision the instance:
+
[.console-input]
[source,bash]
----
rhoas kafka create --name globex --region us-east-1
----
+
[.console-output]
[source,text]
----
{                                                                                                                  
  "cloud_provider": "aws",                                                                                         
  "created_at": "2022-05-23T17:20:03.700415552Z",                                                                  
  "href": "/api/kafkas_mgmt/v1/kafkas/ca5s4gjtq6jlcbnumh5g",                                                       
  "id": "ca5s4gjtq6jlcbnumh5g",                                                                                    
  "instance_type": "developer",                                                                                    
  "kafka_storage_size": "10Gi",                                                                                    
  "kind": "Kafka",                                                                                                 
  "multi_az": false,                                                                                               
  "name": "globex",                                                                                                
  "owner": "rh-bu-cloudservices-tmm",                                                                              
  "reauthentication_enabled": true,                                                                                
  "region": "us-east-1",                                                                                           
  "status": "accepted",                                                                                            
  "updated_at": "2022-05-23T17:20:03.700415552Z"                                                                   
}
----
** To check the status of the kafka instance:
+
[.console-input]
[source,bash]
----
rhoas status
----
+
[.console-output]
[source,text]
----
Service Context Name:   default
Context File Location:  /home/bernard/.config/rhoas/contexts.json

  Kafka
  -----------------------------------------------------------------------------
  ID:                     ca5s4gjtq6jlcbnumh5g
  Name:                   globex
  Status:                 ready
  Bootstrap URL:          globex-ca-s-gjtq-jlcbnumh-g.bf2.kafka.rhcloud.com:443
----

* Create a Kafka topic:
** Create the topic:
+
[.console-input]
[source,bash]
----
rhoas kafka topic create --name globex.tracking --partitions 1
----
** Verify the topics:
+
[.console-input]
[source,bash]
----
rhoas kafka topic list
----
+
[.console-output]
[source,text]
----
  NAME              PARTITIONS   RETENTION TIME (MS)   RETENTION SIZE (BYTES)  
 ----------------- ------------ --------------------- ------------------------ 
  globex.tracking            1   604800000             -1 (Unlimited)         
----

* Connect Streams for Apache Kafka instance.
** Before starting, make sure that you are connected to your OpenShift cluster using the `oc` CLI. 
** To connect your Kafka instance to your project, execute the following command in the terminal:
+
[.console-input]
[source,bash]
----
rhoas cluster connect -n globex
----
** You are asked to select the type of service you want to connect. Select *kafka* and press `enter`.
+
[.console-output]
[source,text]
----
? Select type of service  [Use arrows to move, type to filter]
> kafka
  service-registry
----
** The CLI will prints the *Connection Details* and asks you to confirm. Type `y` and press `enter` to continue.
+
[.console-output]
[source,text]
----
? Select type of service kafka
This command will link your cluster with Cloud Services by creating custom resources and secrets.
In case of problems please execute "rhoas cluster status" to check if your cluster is properly configured

Connection Details:

Service Type:                   kafka
Service Name:                   globex
Kubernetes Namespace:           globex
Service Account Secret:         rh-cloud-services-service-account

? Do you want to continue? (y/N) 
----
** You will be asked to provide a token, which can be retrieved from link:https://console.redhat.com/openshift/token[console.redhat.com/openshift/token]. Navigate to this URL, copy the token to your clipboard, and copy it into your terminal. Press `enter` to continue. 
+
You should see output similar to this:
+
[.console-output]
[source,text]
----
✔️  Token Secret "rh-cloud-services-accesstoken" created successfully
✔️  Service Account Secret "rh-cloud-services-service-account" created successfully

Client ID:     srvc-acct-553dd8d3-e461-411d-a76c-7769bbb5c45d

Make a copy of the client ID to store in a safe place. Credentials won't appear again after closing the terminal.

You will need to assign permissions to service account in order to use it.

You need to separately grant service account access to Kafka by issuing following command

  $ rhoas kafka acl grant-access --producer --consumer --service-account srvc-acct-553dd8d3-e461-411d-a76c-7769bbb5c45d --topic all --group all

✔️  kafka resource "globex" has been created
Waiting for status from kafka resource.
Created kafka can be already injected to your application.

To bind you need to have Service Binding Operator installed:
https://github.com/redhat-developer/service-binding-operator

You can bind kafka to your application by executing "rhoas cluster bind"
or directly in the OpenShift Console topology view.

✔️  Connection to service successful.
----
+
[NOTE]
====
The same command can also be run in a non-interactive way:

[.console-input]
[source,bash]
----
rhoas cluster connect -n globex --service-type kafka --service-name globex --token eyJhbGciOiJ...GDC-cTHCwgmxT-nzM -y
----
====

** To verify that the connection has been successfully created, execute the following `oc` command: 
+
[.console-input]
[source,bash]
----
oc get KafkaConnection -n globex 
----
+
This should return a *KafkaConnection* with the name of your Kafka instance.
+
[.console-output]
[source,text]
----
NAME        AGE
globex      3m42s
----

* Assign permissions to the service account created by the OpenShift Application Services operator:
+
[.console-input]
[source,bash]
----
rhoas kafka acl grant-access --producer --consumer --service-account srvc-acct-553dd8d3-e461-411d-a76c-7769bbb5c45d --topic all --group all -y
----
+
[.console-output]
[source,text]
----
The following ACL rules will be created:

  PRINCIPAL (7)                                    PERMISSION   OPERATION   DESCRIPTION              
 ------------------------------------------------ ------------ ----------- ------------------------- 
  srvc-acct-553dd8d3-e461-411d-a76c-7769bbb5c45d   allow        describe    topic is "*"             
  srvc-acct-553dd8d3-e461-411d-a76c-7769bbb5c45d   allow        read        topic is "*"             
  srvc-acct-553dd8d3-e461-411d-a76c-7769bbb5c45d   allow        read        group is "*"             
  srvc-acct-553dd8d3-e461-411d-a76c-7769bbb5c45d   allow        write       topic is "*"             
  srvc-acct-553dd8d3-e461-411d-a76c-7769bbb5c45d   allow        create      topic is "*"             
  srvc-acct-553dd8d3-e461-411d-a76c-7769bbb5c45d   allow        write       transactional-id is "*"  
  srvc-acct-553dd8d3-e461-411d-a76c-7769bbb5c45d   allow        describe    transactional-id is "*"  

✔️  ACLs successfully created in the Kafka instance "globex"
----

* Bind an application to a Streams for Apache Kafka instance.
** Execute the following command:
+
[.console-input]
[source,bash]
----
rhoas cluster bind -n globex
----
** You are asked to select the application you want to connect to. Select *activity-tracking* and press `enter`. (When repeating for the second application, select *recommendation-engine*)
+
[.console-output]
[source,text]
----
Looking for Deployment resources. Use --deployment-config flag to look for deployment configs
? Please select application you want to connect with  [Use arrows to move, type to filter]
> activity-tracking
  activity-tracking-simulator
  catalog-database
  catalog-service
  globex-ui
  inventory-database
  inventory-service
  recommendation-engine
----
** You are asked to select the type of service you want to connect. Select *kafka* and press `enter`.
+
[.console-output]
[source,text]
----
Looking for Deployment resources. Use --deployment-config flag to look for deployment configs
? Please select application you want to connect with activity-tracking
? Select type of service  [Use arrows to move, type to filter]
> kafka
  service-registry
----
**  The CLI asks you to confirm. Type `y` and press `enter` to continue.
+
[.console-output]
[source,text]
----
Looking for Deployment resources. Use --deployment-config flag to look for deployment configs
? Please select application you want to connect with activity-tracking
? Select type of service kafka
Binding "globex" with "activity-tracking" app
? Do you want to continue? (y/N)
----
+
The CLI produces the following output:
+
[.console-output]
[source,text]
----
Using ServiceBinding Operator to perform binding
✔️  Binding globex with activity-tracking app succeeded
----
+
[NOTE]
====
The command can also be run in a non-interactive way:

[.console-input]
[source,bash]
----
rhoas cluster bind -n globex --app-name activity-tracking --service-type kafka --service-name globex -y
rhoas cluster bind -n globex --app-name recommendation-engine --service-type kafka --service-name globex -y
----
====

:sectnums: